{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Word Analogy using Word Embeddings \n",
    "In the word analogy task, we have $\"a is to b as c is to __\"$. For example is 'boy is to girl as king is to queen' .\n",
    "\n",
    "We find a word $d$, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. <br>\n",
    "For finding $d$ we measure the similarity between $e_b - e_a$ and $e_d - e_c$ using **cosine** similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will be using GLoVe Word Embeddings. Let us load that up.\n",
    "We will have\n",
    "- `words`: set of words in the vocabulary.\n",
    "- `word_to_vec`: dict mapping words to their GloVe vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec = load_glove_vectors('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "We will be using **cosine** similarity for finding the suitable word. We will use $e_b - e_a$ and $e_d - e_c$ as the two vectors to find their cosine, where $e_d$ is searched from all the other words in the vocabulary.\n",
    "\n",
    "Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{Cosine Similarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta)Â $$\n",
    "\n",
    "where $u.v$ is the dot product of two vectors, $||u||_2$ is the norm of the vector $u$, and $\\theta$ is the angle between $u$ and $v$.\n",
    "<br>Norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$\n",
    "\n",
    "This similarity depends on the angle between $u$ and $v$.\n",
    "<br>If $u$ and $v$ are very similar, their cosine similarity will be close to 1<br>\n",
    "If they are dissimilar, the cosine similarity will take a smaller value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the cosine similarity between u and v\n",
    "'''\n",
    "    Arguments:\n",
    "        u(n,) - vector of words            \n",
    "        v(n,) - vector of words \n",
    "    Returns:\n",
    "        cosine_sim - the cosine similarity between u and v\n",
    "'''\n",
    "def find_cosine_similarity(u, v):\n",
    "    distance = 0.0\n",
    "    \n",
    "    # find the dot product between u and v \n",
    "    dot = np.dot(u,v)\n",
    "    # find the L2 norm of u \n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity\n",
    "    cosine_sim = dot/(norm_u)/norm_v\n",
    "    \n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(king, queen) =  0.783904301096\n",
      "cosine_similarity(father, mother) =  0.890903844289\n",
      "cosine_similarity(king - queen, father - mother) =  0.661889473579\n",
      "cosine_similarity(bat, crow) =  0.415745183174\n",
      "cosine_similarity(india - delhi, rome - italy) =  -0.636397420413\n"
     ]
    }
   ],
   "source": [
    "# sample observations\n",
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "king = word_to_vec[\"king\"]\n",
    "queen = word_to_vec[\"queen\"]\n",
    "bat = word_to_vec[\"bat\"]\n",
    "crow = word_to_vec[\"crow\"]\n",
    "india = word_to_vec[\"india\"]\n",
    "italy = word_to_vec[\"italy\"]\n",
    "delhi = word_to_vec[\"delhi\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(king, queen) = \", find_cosine_similarity(king, queen))\n",
    "print(\"cosine_similarity(father, mother) = \", find_cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(king - queen, father - mother) = \",find_cosine_similarity(king - queen, father - mother))\n",
    "print(\"cosine_similarity(bat, crow) = \",find_cosine_similarity(bat, crow))\n",
    "print(\"cosine_similarity(india - delhi, rome - italy) = \",find_cosine_similarity(india - delhi, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the Word analogy task: a is to b as c is to ____\n",
    "def find_analogy(word_a, word_b, word_c, word_to_vec):\n",
    "    # convert words to lower case\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "    \n",
    "    \n",
    "    # find the word embeddings for word_a, word_b, word_c\n",
    "    e_a, e_b, e_c = word_to_vec[word_a], word_to_vec[word_b], word_to_vec[word_c]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    best_word = None                  \n",
    "\n",
    "    # search for word_d in the whole word vector set\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        cosine_sim = find_cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india -> delhi :: japan -> tokyo\n",
      "tall -> taller :: large -> vastly\n"
     ]
    }
   ],
   "source": [
    "examples = [('india', 'delhi', 'japan'), ('tall', 'taller', 'large')]\n",
    "for example in examples:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *example, find_analogy(*example, word_to_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for taking input from the user and doing word analogy task on that\n",
    "def take_input():\n",
    "    print('a --> b :: c --> d')\n",
    "    print('Enter a, b, c words separated by space')\n",
    "    words = input().split(' ')\n",
    "    \n",
    "    best_pick = find_analogy(*words, word_to_vec)\n",
    "    print ('{} -> {} :: {} -> {}'.format( *words, best_pick))\n",
    "    print('Best pick: ' + best_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a --> b :: c --> d\n",
      "Enter a, b, c words separated by space\n",
      "king queen boy\n",
      "king -> queen :: boy -> girl\n"
     ]
    }
   ],
   "source": [
    "take_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**:\n",
    "\n",
    "- The GloVe word embeddings by Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
